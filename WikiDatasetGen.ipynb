{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_files = os.listdir('./wiki_data/data')\n",
    "article_titles = [title[:-4] for title in wiki_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_documents = []\n",
    "wiki_summaries = []\n",
    "titles = []\n",
    "wiki_headers = []\n",
    "wiki_sidebars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents_index(article_lines):\n",
    "    for i in range(len(article_lines)):\n",
    "        if article_lines[i] == 'Contents\\n':\n",
    "            return i\n",
    "    \n",
    "    return -1\n",
    "\n",
    "def parse_summary(article_lines):\n",
    "    index = get_contents_index(article_lines)\n",
    "    \n",
    "    while(len(article_lines[index].split(\" \"))) < 8 and index >= 0:\n",
    "        index -= 1\n",
    "    \n",
    "    summary_indices = []\n",
    "    summary_indices.append(index)\n",
    "    \n",
    "    summary = []\n",
    "    while len(article_lines[index].split(\" \")) > 8 and index >= 0:\n",
    "        summary.insert(0, article_lines[index].strip())\n",
    "        index -= 1\n",
    "        \n",
    "    summary_indices.append(index)\n",
    "        \n",
    "    return summary, summary_indices[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Publication history',\n",
       " 'Creation and inspiration',\n",
       " 'Novels and related works',\n",
       " 'Ian Fleming novels',\n",
       " 'Post-Fleming novels',\n",
       " 'Young Bond',\n",
       " 'The Moneypenny Diaries',\n",
       " 'Adaptations',\n",
       " 'Television',\n",
       " 'Radio',\n",
       " 'Comics',\n",
       " 'Films',\n",
       " 'The Eon Productions films',\n",
       " 'Non-Eon films',\n",
       " 'Music',\n",
       " 'Video games',\n",
       " 'Guns, vehicles and gadgets',\n",
       " 'Guns',\n",
       " 'Vehicles',\n",
       " 'Gadgets',\n",
       " 'Cultural impact',\n",
       " 'Criticisms of James Bond']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HEADERS_TO_IGNORE = [\"See also\", \"References\", \"Bibliography\", \"External links\"]\n",
    "\n",
    "def parse_content_headers(article_lines):\n",
    "    index = get_contents_index(article_lines) + 1\n",
    "    \n",
    "    headers = []\n",
    "    \n",
    "    while True:\n",
    "        line = article_lines[index]\n",
    "        if line[0].isnumeric():\n",
    "            header = \" \".join(line.split(\" \")[1:]).strip()\n",
    "            if header not in HEADERS_TO_IGNORE:\n",
    "                headers.append(header)\n",
    "        elif line[0] != \"\\n\":\n",
    "            break\n",
    "        \n",
    "        index += 1\n",
    "    \n",
    "    \n",
    "    return headers\n",
    "\n",
    "a_lines = open('./wiki_data/data/James Bond.txt').readlines()\n",
    "parse_content_headers(a_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['James Bond',\n",
       " 'Created by',\n",
       " 'Ian Fleming',\n",
       " 'Original work',\n",
       " 'Casino Royale (1953)',\n",
       " 'Print publications',\n",
       " 'Novel(s)',\n",
       " 'List of novels',\n",
       " 'Short stories',\n",
       " 'Films and television',\n",
       " 'Film(s)',\n",
       " 'List of films',\n",
       " 'Short film(s)',\n",
       " 'Happy and Glorious',\n",
       " 'Television series',\n",
       " 'Miscellaneous',\n",
       " 'Portrayers',\n",
       " 'George Baker',\n",
       " 'Pierce Brosnan',\n",
       " 'Daniel Craig',\n",
       " 'Sean Connery',\n",
       " 'Timothy Dalton',\n",
       " 'Bob Holness',\n",
       " 'Michael Jayston',\n",
       " 'George Lazenby',\n",
       " 'Roger Moore',\n",
       " 'Barry Nelson',\n",
       " 'David Niven',\n",
       " 'Toby Stephens']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sidebar_index(article_lines, file):\n",
    "    title = file.strip('.txt')\n",
    "    for i in range(len(article_lines)):\n",
    "        if article_lines[i] == (title + \"\\n\"):\n",
    "            return i\n",
    "    \n",
    "    return -1\n",
    "\n",
    "def parse_right_sidebar(article_lines, file, end_index):\n",
    "    index = get_sidebar_index(article_lines, file)\n",
    "    \n",
    "    if index == -1:\n",
    "        return []\n",
    "    \n",
    "    sidebar = []\n",
    "    \n",
    "    for i in range(index, end_index):\n",
    "        line = article_lines[i]\n",
    "        if line != \"\\n\" and len(line.split(\" \")) <= 3:\n",
    "            sidebar.append(line.strip())\n",
    "    \n",
    "    return sidebar\n",
    "\n",
    "a_lines = open('./wiki_data/data/James Bond.txt').readlines()\n",
    "parse_right_sidebar(a_lines, 'James Bond.txt', 76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in wiki_files:\n",
    "    lines = open('./wiki_data/data/' + file).readlines()\n",
    "    summary, summary_indices = parse_summary(lines)\n",
    "    lower_bound, upper_bound = summary_indices\n",
    "    \n",
    "    headers = parse_content_headers(lines)\n",
    "    \n",
    "    sidebar = parse_right_sidebar(lines, file, lower_bound)\n",
    "\n",
    "    if len(summary) > 0:\n",
    "        titles.append(file)\n",
    "        \n",
    "        wiki_summaries.append('. '.join(summary))\n",
    "\n",
    "        cleaned_lines = [lines[i].strip() for i in range(len(lines)) if lines[i].strip() != '' and (i < lower_bound or i > upper_bound)]\n",
    "        wiki_documents.append('. '.join(cleaned_lines))\n",
    "        \n",
    "        wiki_headers.append(' --- '.join(headers))\n",
    "        \n",
    "        wiki_sidebars.append(' --- '.join(sidebar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.DataFrame(columns=['title', 'summary', 'document', 'headers', 'sidebar'])\n",
    "df_3['title'] = pd.Series(titles)\n",
    "df_3['summary'] = pd.Series(wiki_summaries)\n",
    "df_3['document'] = pd.Series(wiki_documents)\n",
    "df_3['headers'] = pd.Series(wiki_headers)\n",
    "df_3['sidebar'] = pd.Series(wiki_sidebars)\n",
    "df_3.to_csv('wiki_old_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
